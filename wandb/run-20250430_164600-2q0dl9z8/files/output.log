wandb 运行已初始化:
  实体: electrixoul-tsinghua-university
  项目: mnist-sae-training
  运行名称: mnist-sae-train-k50
  运行 URL: https://wandb.ai/electrixoul-tsinghua-university/mnist-sae-training/runs/2q0dl9z8
加载MNIST数据集...
MNIST训练集: 60000 样本
MNIST测试集: 10000 样本
模型结构:
SparseAutoencoder(
  (encoders): ModuleList(
    (0-4): 5 x Linear(in_features=784, out_features=1024, bias=True)
  )
)
初始模型保存到: mnist_sae_models/mnist_sae_initial.pth
开始训练SAE模型...
轮次 1/5
self.device:  cpu
consensus_loss:  tensor(2.1921e-06, grad_fn=<MulBackward0>)
consensus_loss:  tensor(8.7649e-06, grad_fn=<MulBackward0>)
/home/t/anaconda3/envs/sae_eeg/lib/python3.10/site-packages/torch/autograd/graph.py:824: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
consensus_loss:  tensor(1.9706e-05, grad_fn=<MulBackward0>)
consensus_loss:  tensor(3.4996e-05, grad_fn=<MulBackward0>)
consensus_loss:  tensor(5.4610e-05, grad_fn=<MulBackward0>)
consensus_loss:  tensor(7.8527e-05, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0001, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0001, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0002, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0002, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0003, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0003, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0004, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0004, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0005, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0005, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0006, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0007, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0008, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0008, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0009, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0010, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0011, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0012, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0013, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0014, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0014, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0015, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0016, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0017, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0018, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0019, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0021, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0022, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0023, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0024, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0025, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0026, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0027, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0028, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0029, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0031, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0032, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0033, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0034, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0035, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0036, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0038, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0039, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0040, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0041, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0042, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0043, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0044, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0046, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0047, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0048, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0049, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0050, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0051, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0052, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0053, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0054, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0055, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0056, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0057, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0058, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0059, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0060, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0061, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0062, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0063, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0063, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0064, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0065, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0066, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0066, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0067, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0068, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0068, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0069, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0069, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0070, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0070, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0071, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0071, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0072, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0072, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0072, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0073, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0073, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0073, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0073, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0073, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0073, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0073, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0073, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0073, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0073, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0073, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0073, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0073, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0073, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0073, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0073, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0073, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0073, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0073, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0072, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0072, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0072, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0072, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0072, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0072, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0072, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0072, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0072, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0072, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0072, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0071, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0071, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0071, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0071, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0071, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0071, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0071, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0071, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0071, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0071, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0071, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0071, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0070, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0070, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0070, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0070, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0070, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0070, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0070, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0070, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0070, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0070, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0070, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0070, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0070, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0069, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0069, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0069, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0069, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0069, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0069, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0069, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0069, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0069, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0069, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0069, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0069, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0069, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0069, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0068, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0068, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0068, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0068, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0068, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0068, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0068, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0068, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0068, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0068, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0068, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0068, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0068, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0068, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0068, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0067, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0067, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0067, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0067, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0067, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0067, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0067, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0067, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0067, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0067, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0067, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0067, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0067, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0067, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0067, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0066, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0066, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0066, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0066, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0066, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0066, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0066, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0066, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0066, grad_fn=<MulBackward0>)
consensus_loss:  tensor(0.0066, grad_fn=<MulBackward0>)
Traceback (most recent call last):
  File "/home/t/workspace/lab_work/sae_demo/mnist_sae_train.py", line 306, in <module>
    main()
  File "/home/t/workspace/lab_work/sae_demo/mnist_sae_train.py", line 245, in main
    trainer.train(train_loader, 1)
  File "/home/t/workspace/lab_work/sae_demo/custom_sae_trainer.py", line 110, in train
    optimizer.step()
  File "/home/t/anaconda3/envs/sae_eeg/lib/python3.10/site-packages/torch/optim/optimizer.py", line 485, in wrapper
    out = func(*args, **kwargs)
  File "/home/t/anaconda3/envs/sae_eeg/lib/python3.10/site-packages/torch/optim/optimizer.py", line 79, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/t/anaconda3/envs/sae_eeg/lib/python3.10/site-packages/torch/optim/adam.py", line 246, in step
    adam(
  File "/home/t/anaconda3/envs/sae_eeg/lib/python3.10/site-packages/torch/optim/optimizer.py", line 147, in maybe_fallback
    return func(*args, **kwargs)
  File "/home/t/anaconda3/envs/sae_eeg/lib/python3.10/site-packages/torch/optim/adam.py", line 933, in adam
    func(
  File "/home/t/anaconda3/envs/sae_eeg/lib/python3.10/site-packages/torch/optim/adam.py", line 456, in _single_tensor_adam
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/t/workspace/lab_work/sae_demo/mnist_sae_train.py", line 306, in <module>
    main()
  File "/home/t/workspace/lab_work/sae_demo/mnist_sae_train.py", line 245, in main
    trainer.train(train_loader, 1)
  File "/home/t/workspace/lab_work/sae_demo/custom_sae_trainer.py", line 110, in train
    optimizer.step()
  File "/home/t/anaconda3/envs/sae_eeg/lib/python3.10/site-packages/torch/optim/optimizer.py", line 485, in wrapper
    out = func(*args, **kwargs)
  File "/home/t/anaconda3/envs/sae_eeg/lib/python3.10/site-packages/torch/optim/optimizer.py", line 79, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/t/anaconda3/envs/sae_eeg/lib/python3.10/site-packages/torch/optim/adam.py", line 246, in step
    adam(
  File "/home/t/anaconda3/envs/sae_eeg/lib/python3.10/site-packages/torch/optim/optimizer.py", line 147, in maybe_fallback
    return func(*args, **kwargs)
  File "/home/t/anaconda3/envs/sae_eeg/lib/python3.10/site-packages/torch/optim/adam.py", line 933, in adam
    func(
  File "/home/t/anaconda3/envs/sae_eeg/lib/python3.10/site-packages/torch/optim/adam.py", line 456, in _single_tensor_adam
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
KeyboardInterrupt